\renewcommand*\chapterpagestyle{scrheadings}
\chapter{Implementation}

\section{Spring Boot Prototype}
The initial implementation of the backend service was developed in Kotlin using the Spring Boot framework\footnote{Spring Framework \cite{spring}},
a Java-based platform designed to provide foundational infrastructure for web applications.
The service featured a prototype REST endpoint accessible at \texttt{http://localhost:8080/process},
which accepted textual input as an interim solution prior to the integration of audio processing capabilities.
This endpoint utilized pattern-matching logic to direct weather-related queries to a dedicated weather service,
while all other queries were processed using the GPT-4 language model.

\section{Core Functionality}
As outlined in Section \ref{sec:design}, the system employs a modular microservices architecture where components can be swapped or updated independently.
This is achieved through Rust's trait system, which defines shared functionality for different structs, similar to interfaces in object-oriented languages.
Each microservice (e.g., audio transcription, weather data processing) implements a specific trait, ensuring consistent behavior across implementations.
The \texttt{async\_trait} crate provides support for asynchronous methods when both the trait and its implementation are annotated with \texttt{\#[async\_trait]}.

\subsection{Recording}

\subsubsection{Initial Idea}
The recording microservice was originally designed with synchronous methods for capturing audio using a CPAL stream.
These methods were exposed through the \texttt{RecordingService} trait, which defined two primary methods: \texttt{start},
to initiate audio capture, and \texttt{stop}, to retrieve the recorded audio bytes.
Initially, these methods were synchronous, requiring the server to block while waiting for the user to finish recording.
However, supporting a remote recording solution necessitated transforming \texttt{RecordingService} into a fully asynchronous trait to facilitate network communication.
Integrating CPAL's inherently synchronous and non-thread-safe \texttt{Stream} into an asynchronous context introduced significant implementation challenges.
CPAL's \texttt{Stream} type is neither \texttt{Send} nor \texttt{Sync}, preventing safe usage across threads and within async contexts.

\subsubsection{Temporary Unsafe Implementation}
To circumvent the concurrency constraints temporarily, the recording component initially utilized an unsafe wrapper named \texttt{UnsafeSendSync}.
This wrapper forcibly implemented the \texttt{Send} and \texttt{Sync} traits for CPAL's stream, satisfying compiler requirements despite inherent thread-safety violations:

\begin{minted}{rust}
pub struct UnsafeSendSync<T>(pub T);
unsafe impl<T> Sync for UnsafeSendSync<T> {}
unsafe impl<T> Send for UnsafeSendSync<T> {}
\end{minted}

This approach allowed the system to function temporarily by bypassing Rust's concurrency guarantees, but it was inherently risky and only used in production for a single day. The unsafe implementation was a stopgap measure until a more robust solution could be developed.

\subsubsection{Final Implementation: Isolation of Synchronous Recording}
The final implementation addressed the safety concerns by isolating the synchronous CPAL stream operation into a dedicated synchronous thread, separate from the async interface.
The dedicated thread continuously captures audio samples from CPAL and writes them into a shared ring buffer implemented as a \texttt{VecDeque}.
Access to the buffer is synchronized using a mutex, and an atomic counter keeps track of the total number of samples recorded.

The asynchronous \texttt{LocalRecorder} struct implements the \texttt{RecordingService} trait without directly accessing the CPAL stream.
Instead, it interacts solely with the shared state—the mutex-protected ring buffer and atomic counter—to determine which audio data segments should be returned.
Error propagation is handled by transmitting any errors from the synchronous CPAL thread to the asynchronous interface via a Tokio mpsc channel.
This design ensures asynchronous methods can detect and return relevant errors without violating Rust's safety constraints.

\section{Speech-to-Text Transcription}
Speech-to-text (STT) systems convert spoken language into written transcriptions through a layered computational process.
These systems mimic human auditory cognition by combining signal filtering, pattern recognition, and contextual analysis.
The workflow progresses sequentially, transforming raw sound waves into structured text while balancing speed and accuracy.

\subsection{Audio Preprocessing}
Raw audio recordings contain background noise and volume inconsistencies that obscure speech signals.
To address this, preprocessing applies noise reduction algorithms, such as spectral subtraction,
which isolates vocal frequencies by subtracting non-speech patterns (e.g., wind or keyboard clicks).
Normalization then stabilizes volume spikes, ensuring whispers and shouts are processed uniformly.
Finally, feature extraction converts the cleaned audio into machine-readable numerical data.

Mel Frequency Cepstral Coefficients (MFCCs) are widely used for this purpose ---  they mathematically represent
the "shape" of speech by encoding pitch, tone, and rhythm into a compact vector.
Alternatively, spectrograms visualize audio as heatmaps of frequency intensity over time,
allowing convolutional neural networks to treat speech as a spatial pattern.

\subsection{Neural Network Analysis}
Preprocessed audio enters a neural network trained to map acoustic features to linguistic units.
Recurrent neural networks (RNNs), particularly long short-term memory (LSTM) variants, excel here
—--
they process audio chronologically, maintaining memory of prior sounds to interpret context
(e.g., distinguishing "ice cream" from "I scream" through syllable relationships).

Transformers, a newer architecture, bypass sequential analysis entirely.
Instead, they use self-attention mechanisms to dynamically weigh connections between
disparate audio segments, enabling faster parallel processing of long phrases.
Hybrid models often combine these approaches, using convolutional layers to detect phonemes
before feeding them to transformers for sentence-level interpretation.

\subsection{Decoding Ambiguity}
Network outputs are probabilistic --- for example, a sound midway between "cat" and "cap" might receive
a 55\% likelihood for "cat." Decoding algorithms resolve these uncertainties using language models
that prioritize statistically common word sequences. The phrase "the cat sat" would override
"the cap sat" due to its higher frequency in training data.

Connectionist Temporal Classification (CTC) aligns variable-length audio with fixed text outputs,
handling speech rate variations by collapsing repeated characters (e.g., converting "ssssat" to "sat").
Attention-based decoders refine this by focusing computational resources on acoustically ambiguous segments,
similar to how humans re-listen to mumbled words in conversation.

\subsection{Contextual Postprocessing}
Raw transcriptions lack punctuation and capitalization, so STT systems apply rules inferred from audio prosody.
A sustained pause might insert a comma, while rising pitch at sentence end adds a question mark.
Domain-specific dictionaries further refine outputs --- medical STT systems might prioritize "metacarpal" over
"meta carpal" based on context, reducing homophone errors.

Modern implementations optimize for real-world use through edge computing, where processing occurs locally
on smartphones rather than cloud servers, ensuring privacy for sensitive conversations.
Open-source toolkits like Mozilla DeepSpeech democratize development, allowing customization for niche accents
or specialized vocabularies (e.g., transcribing technical jargon in engineering lectures).

\subsection{Rust Trait}
A trait was defined in Rust with a single function \texttt{transcribe} that accepts a \texttt{Bytes} parameter and returns a \texttt{String} result.
\begin{minted}{rust}
pub trait TranscriptionService: Send + Sync {
    async fn transcribe(&self, audio: &Bytes) -> Result<String>;
}
\end{minted}

\subsection{Whisper}
OpenAI Whisper provides speech recognition through a transformer model trained on
680,000 hours of multilingual web data. Its architecture processes audio as
log-Mel spectrograms --- matrices representing sound frequency intensity over 25ms windows ---
which preserve phonetic distinctions better than raw waveforms. The spectrogram
generation applies triangular filters spaced according to Mel scale, approximating
human hearing sensitivity across 80 frequency bands.

A 1.5-billion parameter transformer encoder-decoder structure then maps these
spectrograms to text tokens. Encoder layers create latent representations through
multi-head self-attention, identifying relationships between distant phonemes
(e.g., connecting verbs to their subjects across clauses). The decoder generates
text autoregressively, using cross-attention to align phoneme patterns with
likely word sequences --- critical for distinguishing homophones like "write" and "right"
through surrounding context.

Whisper's training regimen combines weakly supervised learning from noisy web captions
with pseudo-label refinement. During inference, beam search with length normalization
balances transcription accuracy and fluency, while temperature sampling introduces
controlled randomness for non-deterministic outputs. The model handles code-switching
by jointly training on 96 languages, dynamically weighting language-specific token
distributions based on spectral cues.

Integration via the whisper-rs Rust binding processes audio through non-overlapping
30-second chunks using sequential windowing:

\begin{minted}{rust}
pub struct LocalWhisperClient {
    context: Arc<WhisperContext>,
}

impl LocalWhisperClient {
    pub fn new(model: impl Into<String>, use_gpu: bool) -> Result<Self> {
        let mut params = WhisperContextParameters::default();
        params.use_gpu = use_gpu;
        let context = Arc::new(WhisperContext::new_with_params(&model.into(), params)?);
        Ok(Self { context })
    }
}
\end{minted}

\subsection{Parsing}
Once the audio is transcribed into text, it is passed to a parser.
The parser can be a simple pattern matcher or an advanced LLM service such as Rasa.

\begin{minted}{rust}
pub trait ParsingService: Send + Sync {
    async fn parse(&self, input: &str) -> Result<Action>;
}
\end{minted}

This service accepts a string input, which it receives after the transcription process, and returns a structured \texttt{Action}.

\begin{minted}{rust}
pub struct Action {
    pub intent: Intent,
    pub entities: Vec<Entity>,
    pub text: String,
}
\end{minted}

An \texttt{Action} is a combination of intent and entities.
The intent represents the action that the parser determines should be performed, along with a confidence level.
This is particularly important for natural language processors using LLMs, where results are probabilistic rather than deterministic.

\begin{minted}{rust}
pub struct Intent {
    pub name: IntentKind,
    pub confidence: Option<f32>,
}

pub enum IntentKind {
    LlmQuery,
    WeatherQuery,
}
\end{minted}

Entities function similarly to variables. In a pattern-matching parser, they are straightforward variables,
while an LLM parser extracts what it deems the most likely desired variable,
such as the location in a weather request or the brightness level for adjusting lights.
When using an LLM parser, entities are also paired with a confidence level.
This confidence is typically very high (around 99.99\%), but it is wrapped in an \texttt{Option}
to account for scenarios where confidence is not applicable,
such as with a pattern-matching parser or a geopolitical entity extractor in an LLM processor.

\begin{minted}{rust}
pub struct Entity {
    pub entity: String,
    pub value: String,
    pub confidence: Option<f32>,
}
\end{minted}

\section{Individual Features}

\subsection{Geocoding}
The geocoding service is an internal component that converts textual location names into geographical coordinates, but it is not available to the user.

\begin{minted}{rust}
pub trait GeocodingService: Send + Sync {
    async fn request(&self, address: &str) -> Result<GeocodeResponse>;
}
\end{minted}

The service returns standardized location data including the formatted name and coordinates:

\begin{minted}{rust}
pub struct GeocodeResponse {
    pub name: String,
    pub lat: String,
    pub lon: String,
}
\end{minted}

An implementation of the \texttt{GeocodingService} utilizes the OpenStreetMap Nominatim API
to convert unstructured location names into standardized geographical coordinates.
These coordinates serve as input parameters for subsequent \texttt{WeatherService} requests.
The API processes natural language location descriptions and returns precise latitude and longitude values along with a formatted location name.
This formatted name is later used as a display name when presenting weather response data to the client.

\subsection{Weather}
The weather service retrieves weather data using geographical coordinates:

\begin{minted}{rust}
pub trait WeatherService: Send + Sync {
    async fn request(&self, geocode: GeocodeResponse) -> Result<String>;
}
\end{minted}

The response data is structured according to the OpenWeather API\footnote{OpenWeather API \cite{openweathermap}} specification,
containing temperature, humidity, and weather condition descriptions:

\begin{minted}{rust}
pub struct WeatherResponse {
    pub main: Main,
    pub weather: Vec<Weather>,
}

pub struct Main {
    pub temp: f64,
    pub humidity: u8,
}

pub struct Weather {
    pub description: String,
}
\end{minted}

The OpenWeather API serves as the system's primary weather data provider, offering a free tier with 1,000 API calls per day.
It processes the geographic coordinates received from the \texttt{GeocodeResponse} generated by the \texttt{GeocodingService}.
Each API request requires mandatory parameters including latitude, longitude, and an authentication key, along with optional configuration parameters.
These optional parameters allow customization of the response through \texttt{units} (standard, metric, or imperial measurements),
\texttt{exclude} (response field filtering), and \texttt{lang} (response language selection).

\subsection{Timers}
Timers were first implemented in-memory using a tokio thread and by simply sleeping for the duration of the timer.

\begin{minted}{rust}
pub trait TimerService: Send + Sync {
    async fn set(&self, duration: Duration, description: String) -> Result<String>;
}
\end{minted}

Each timer requires an \texttt{std::time::Duration} and a description, where the description is what is said or shown when the timer rings.
