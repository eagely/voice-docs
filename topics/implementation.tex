\cleardoubleemptypage
\renewcommand*\chapterpagestyle{scrheadings}

\chapter{Implementation}

\subsection{Communication}
Communication between frontend and backend constitutes a critical architectural consideration.
Several communication paradigms were trialed during development.
Initially, HTTP with plain text was employed under the assumption that the frontend would handle audio recording.
However, attempts to implement this in Qt revealed numerous complications.
Consequently, a backend microservice for audio recording was introduced, rendering HTTP suboptimal.
A shift was made to raw TCP, which incurs less overhead and permits efficient full-duplex transmission of brief commands.
Despite its efficiency, raw TCP with a binary protocol for text, audio, commands, and configurations proved cumbersome.
Therefore, a final transition to WebSockets was implemented.
WebSockets, essentially a TCP wrapper with structured headers, offer a clean means of distinguishing data types,
thus simplifying system communication without compromising performance.

\section{Spring Boot Prototype}
The backend was initially developed using Spring Boot, as previously described in \ref{subsec:language-and-framework}.
The prototype featured a REST endpoint, which was accessible at \texttt{http://localhost:8080/process}.
\begin{minted}{kotlin}
@PostMapping("/process")
fun process(@RequestBody content: String): Mono<ResponseEntity<String>> {
    return parsingService.parse(content).map { ResponseEntity.ok(it) }
}
\end{minted}
It would accept an HTTP POST request with a string in its body, which was the input sent by the user.
This was the first implementation, and it did not support audio yet, in order to keep it simple and iterate quickly.
A pattern matching parser was implemented using Kotlin's \texttt{when} statement, which is similar to a switch-case in other languages.
Below is a demonstrative, simplified version of this parser:
\begin{figure}[H]
\begin{minted}{kotlin}
when {
    "units" in input -> unitsService.getUnitsInfo()
    "weather" in input -> {
        geocodingService.getLocation(input).map { (town, lat, lon) ->
            weatherService.getForecast(lat, lon)

        }
    }

    else -> openAIService.getCompletion(input).map {
                it.choices.first().message.content!!
            }
}
\end{minted}
\end{figure}
It had three services implemented, those being:
\begin{itemize}
  \item Units Service: Returns the units that are in use (metric or imperial)
  \item Weather Service: Queries the OpenWeatherMap API using coordinates from the OpenStreetMap Nominatim API
  \item Open AI Service: Queries an LLM from OpenAI if the parser does not find the pattern of any other service
\end{itemize}

\section{Rewriting it in Rust}
Because Kotlin runs on the Java Virtual Machine\footnote{Java Virtual Machine \cite{jvm}},
it does not offer particularly good performance.
Since the plan was to run the voice assistant on a Raspberry Pi,
a decision was made early in the development of the project
to abandon the current backend and rewrite it in Rust ---
a significantly more performant alternative that does not compromise on ergonomics.
Rust compiles to native machine code and is therefore nearly as fast as C or C++,
only with slow compilation times and some overhead introduced by the borrow checker ---
both of which can be disregarded when compared to the limitations of the Java Virtual Machine.

Rust takes an innovative approach to systems programming by combining low-level control with modern language features
that do not compromise on ergonomics. It uses errors as values using the \texttt{Result} enum:

\begin{figure}[H]
    \begin{minted}{rust}
enum Result<T, E> {
    Ok(T),
    Err(E),
}
    \end{minted}
\end{figure}

And by utilizing its powerful pattern-matching capabilities, this becomes an awesome way to avoid almost all runtime errors
in a simple and elegant way.

In addition to that, Rust has a very powerful type system and algebraic data types, involving structs, which are similar to classes
in object-oriented programming languages and are \textit{product types} in type theory terms; Rust also offers enums, which
are \textit{sum types} in type theory. Structs and enums in Rust are more powerful than in other programming languages,
as they can hold multiple values, and even named fields, not just singular primitive types.
Using this concept, the following is possible:
\begin{minted}{rust}
enum Foo {
    Bar(f64, f64, f64),
    Baz(String, String, String),
    Qux {
        quux: f64,
        quuz: f64,
    },
}

struct Bar {
    Baz: f64,
    Qux: (f64, String, i128),
}
\end{minted}

These data types can then be used in \texttt{match} statements, as well is the \texttt{if let} statement
for really powerful and fun-to-use pattern matching.

Interfaces are a very common thing in programming languages nowadays ---
Rust does not have interfaces, it has traits: they are very similar to interfaces
in the sense that they allow specifying that a certain struct implements a certain feature,
or multiple certain features and allows for the a trait to be a function parameter,
not a specific implementation. However, traits differ with scoping and the defition of which traits are implemented for a certain struct:
to implement an interface for a certain class, one has to specify that the class implements that interface,
however, to implement a trait, one can just write \texttt{impl Trait for Struct {}} and then import the trait wherever it needs to be used.

Another elevator pitch of Rust is its focus on memory safety; Rust has a system called the \texttt{Borrow Checker}, which
does what you think it does: It checks borrows --- Rust has a concept called \texttt{Ownership and Borrowing},
where a variable always has an owner and can either be immutably borrowed any number of times or mutably borrowed once at the same time.
Borrowing a value means holding a reference to it, but not owning it, hence not being able to pass it as an owned value
to another function without first cloning it. Another feature of the borrow checker are lifetimes:
Any variable is borrowed only until its lifetime expires and any value a variable holds must live
at least as long as it, otherwise a compile-time error will be thrown.

At first the borrow checker may seem excessively limiting, however, when properly learned and managed,
it creates an almost perfect guarantee of memory safety and the absence of data racing unless the unsafe keyword is used,
which allows the developer to bypass Rust's memory safety systems for things that need to be done but inherently cannot be done safely,
such as writing to a specific register in memory on a microcontroller, which could, for example, control an LED, or set up an interrupt service routine.
Another example of the unsafe keyword, which was used for this project, is the implementation of \texttt{UnsafeSendSync} for audio recording below.

\section{Audio Recording}
The original plan for audio recording was to require any frontend implementations to record audio and send raw audio data to the backend.
Attempts were made to implement this in the reference frontend in Qt/C++, but to no avail --- implementing an audio recorder in Qt
requires extensive knowledge of the underlying operating system and audio drivers, and the documentation was not complete enough to cover every edge case.
This led to the idea that audio recording could be a microservice like any other, but it would have to be synchronous, since audio recording
is inherently a blocking process --- you can not do anything while recording and you can not record twice at the same time using the same device.
However, one could argue that there may be multiple clients connected that want to use a microservice which records audio remotely,
on different microphones (such as microphones on smart home nodes), this would again require an asynchronous microservice implementation.
The issue was that a microservice uses a Rust trait as its specification, and this trait can not support synchronous and asynchronous implementations simultaneously.
A decision had to be made: Is it better to wrap the local recording implementation into an asynchronous API, or make the asynchronous implementation blocking.
Ultimately, while blocking the asynchronous remote recorder would be much easier, it would also heavily slow down a system with multiple clients connected,
so the decision was made to turn the API of the local recorder asynchronous.

\subsection{Local Recorder Implementation}
CPAL\footnote{Cross-Platform Audio Library \cite{cpal}} is a Rust library that provides easy access to the audio interface of the system,
allowing one to record audio from Rust code. Using Hound\footnote{Hound \cite{hound}}, the output of CPAL could be encoded into a WAV file
to be sent to the transcription microservice. However, integrating CPAL's inherently synchronous and non-thread safe \texttt{Stream} into an asynchronous API
proved to be challenging, as Rust does not allow synchronous code to run inside asynchronous context, due to concerns for memory safety and avoiding data racing.

\subsection{Temporary Unsafe Implementation}
CPAL's \texttt{Stream} is not thread-safe because it does not implement the \texttt{Send} and \texttt{Sync} traits.
A very intuitive but improper way to circumvent the constraints is to simply implement an unsafe wrapped for \texttt{Send} and \texttt{Sync};
this was done as an interim stopgap measure to allow work to be done on other parts of the project while deliberating about how to implement a proper asynchronous wrapper.
This inherently violates the compiler's thread-safety guarantees, but is acceptable as long as there is only one instance of the local recorder (so only one client).

\begin{minted}{rust}
pub struct UnsafeSendSync<T>(pub T);
unsafe impl<T> Sync for UnsafeSendSync<T> {}
unsafe impl<T> Send for UnsafeSendSync<T> {}
\end{minted}

\subsection{Isolation of Synchronous Recording}
The day after implementing the \texttt{UnsafeSendSync} wrapper for CPAL's \texttt{Stream}, it was decided to simply isolate the \texttt{Stream}
into its own dedicated synchronous thread, separate from the async interface, and communicate using a shared buffer, which could safely be sent across threads. 
This buffer was a 10-minute long ring buffer, which means that after 10 minutes of recording, it would overwrite the first samples of the recording.
10 minutes was chosen as the cut-off because it seems unrealistic that someone would talk for more than 10 minutes straight to a voice assistant.
Access to this buffer is synchronized using a mutex, and an atomic counter that keeps track of the total number of samples that have been recorder thus far.
Errors that are thrown inside the synchronous thread are sent to the asynchronous interface via a Tokio multi-producer single-consumer channel and propagated upstream.

\subsection{Wake Word Detection}
Wake words enable hands-free activation of voice assistants through specific phrases.
Commercial systems commonly use phrases like "Hey Siri" or "Okay Google".
The implementation utilizes the Porcupine library\footnote{Porcupine \cite{porcupine}} for wake word detection.
"Ferris" was selected as the wake word, referencing the Rust mascot.

The detection process is implemented through several code components:

\begin{minted}{rust}
fn process_audio_data(
    data: &[f32],
    frame_buffer: &mut Vec<i16>,
    porcupine: &Porcupine,
    wake_word_enabled: &Arc<AtomicBool>,
    is_recording: &Arc<AtomicBool>,
) {
\end{minted}

This function accepts raw audio data and maintains a frame buffer.
A Porcupine detector instance and atomic flags enable state management.
These parameters allow audio processing and recording state control.

\begin{minted}{rust}
for &sample in data {
    let sample_i16 = (sample * 32767.0) as i16;
    frame_buffer.push(sample_i16);
}
\end{minted}

The previously floating-point audio samples are converted to 16-bit integers,
because Porcupine requires 16-bit Pulse Code Modulation (PCM),
which uses a fixed-point representation with 16-bit precision.
Each converted sample is added to the frame buffer for processing.

\begin{minted}{rust}
if frame_buffer.len() >= porcupine.frame_length() as usize {
    if let Ok(keyword_index) = porcupine.process(frame_buffer) {
\end{minted}

When the buffer reaches sufficient size, Porcupine processes it.
The process method returns a keyword index upon successful detection.

\begin{minted}{rust}
if keyword_index >= 0 && wake_word_enabled.load(Ordering::Relaxed) {
    is_recording.store(true, Ordering::Relaxed);
}
frame_buffer.clear();
\end{minted}

A non-negative keyword index indicates successful wake word detection.
If detection is enabled, the system transitions to recording mode.
The frame buffer is cleared afterward to prepare for the next batch.
This cycle continues, allowing continuous monitoring for wake word occurrences.

\section{Speech-to-Text Transcription}

\subsection{Audio Preprocessing}
Raw audio often contains noise and inconsistent volume levels.
Preprocessing techniques, such as spectral subtraction, isolate vocal frequencies and reduce background noise.
Normalization ensures volume consistency.
The cleaned audio is then transformed into numerical features
using methods like the Mel Frequency Cepstrum\footnote{Mel Frequency Cepstral \cite{mfc}} or spectrograms.

\subsection{Neural Network Analysis}
Preprocessed features are input into neural networks trained to map acoustic signals to text.
Recurrent neural networks\footnote{Recurrent Neural Network \cite{rnn}},
particularly Long short-term memory\footnote{Long short-term memory \cite{lstm}} variants, process audio sequentially, retaining context over time.
Transformers, using self-attention\footnote{Self-attention \cite{self-attention}}, model relationships across the entire sequence in parallel.
Hybrid models often combine convolutional layers for phoneme recognition with transformers for broader interpretation.

\subsection{Decoding Ambiguity}
Neural network outputs are inherently probabilistic, which introduces ambiguity in the transcribed text.
To resolve this uncertainty, decoding algorithms integrate statistical language models
that leverage contextual information to select the most likely transcription.
When dealing with variable-length inputs and timing discrepancies,
Connectionist Temporal Classification \footnote{Connectionist Temporal Classification \cite{ctc}}
is employed to collapse repeated predictions and blank labels, providing a preliminary aligned sequence.
This output is then further refined by attention-based models,
which selectively focus on ambiguous segments to produce a more coherent and accurate final transcription.

\subsection{Rust Trait}
The transcription service is defined via a Rust trait:

\begin{minted}{rust}
pub trait TranscriptionService: Send + Sync {
    async fn transcribe(&self, audio: &Bytes) -> Result<String>;
}
\end{minted}

\subsection{OpenAI Whisper}
OpenAI Whisper employs a transformer model trained on 680,000 hours of multilingual data.
Audio is converted into log-Mel spectrograms, which encode phonetic features.
A transformer encoder-decoder processes these to produce text.

Integration in Rust uses \texttt{whisper-rs}:

\begin{minted}{rust}
pub struct LocalWhisperClient {
    context: Arc<WhisperContext>,
}

impl LocalWhisperClient {
    pub fn new(model: impl Into<String>, use_gpu: bool) -> Result<Self> {
        let mut params = WhisperContextParameters::default();
        params.use_gpu = use_gpu;
        let context = Arc::new(WhisperContext::new_with_params(&model.into(), params)?);
        Ok(Self { context })
    }
}
\end{minted}

\section{Parsing}
Parsing is the process of analyzing text input and converting it
into a structured representation of its meaning --- in order to determine which actions should be performed.
This can be accomplished either through rule-based methods
or via Natural Language Understanding (NLU) approaches.
Rule-based parsing involves matching specific patterns in the input ---
using techniques such as regular expressions or Rust's \texttt{match} statement ---
to extract structured data.
In contrast, NLU-based parsing leverages machine learning models,
often using transformer architectures, to detect intent and extract entities from natural language.

\subsection{Action}
An \texttt{Action} encapsulates the parsed result, including intent, entities, and the original input text:

\begin{minted}{rust}
pub struct Action {
    pub intent: Intent,
    pub entities: Vec<Entity>,
    pub text: String,
}
\end{minted}

The intent and entities may optionally carry confidence scores:

\begin{minted}{rust}
pub struct Intent {
    pub name: IntentKind,
    pub confidence: Option<f32>,
}

pub enum IntentKind {
    LlmQuery,
    SetTimer,
    WeatherQuery,
    DecreaseVolume,
    IncreaseVolume,
    SetVolume,
    CloseWindow,
    MaximizeWindow,
    MinimizeWindow,
    SwitchWorkspace,
    ShowDesktop,
    Other(String),
}
\end{minted}

\subsection{Pattern Matching}
A rule-based parser was implemented using Rust's \texttt{match} statement.
It identified known patterns in user input, returning corresponding intents and entities in the form of an \texttt{Action}.

\subsection{NLU-Based Parser}
For more adaptable parsing, an NLU-based solution was implemented using Rasa\footnote{Rasa \cite{rasa}}.
Only the NLU component was used; dialogue handling was external.
Text inputs underwent tokenization and feature extraction before being processed by the DIETClassifier,
a transformer model that performs both intent classification and entity extraction.

Entities were extracted using the BIO tagging scheme, enhanced with spaCy's \texttt{SpacyEntityExtractor}.
spaCy models performed named entity recognition using statistical techniques trained on corpora such as OntoNotes.
The final output included the identified intent with a confidence score and a list of labeled entities.
This output was transformed into an \texttt{Action} object for downstream use.

\section{Geocoding}
The geocoding service translated text-based location names into geographic coordinates.
It processed \texttt{GeocodeResponse} objects from \texttt{GeocodingService}.
Each request required latitude, longitude, and an API key, with optional parameters for \texttt{units}, \texttt{exclude}, and \texttt{lang}.

\section{Timers}
Timers are among the first features associated with a voice assistant,
making proper implementation essential.
Timer functionality is achieved by spawning a new Tokio task that waits for the specified duration;
upon expiration, a desktop notification is sent to indicate that the timer has finished.

\begin{minted}{rust}
pub trait TimerService: Send + Sync {
    async fn set(&self, duration: Duration, description: String) -> Result<String>;
}
\end{minted}
Each timer requires a \texttt{Duration} and a description,
which specifies the output message when the timer completed.

\section{Raspberry Pi Setup}
The voice assistant was developed for deployment on a Raspberry Pi 4,
intended to function as a dedicated node in a smart home environment.
Given the device's limited resources, particular care was taken to reduce local computational load.
Where possible, services were designed with remote execution in mind,
as to not put any heavy load on the low-end hardware.
Alpine Linux\footnote{Alpine Linux \cite{alpine}}
was chosen as the operating system for its minimalism, fast boot times, and low memory usage.

\subsection{Installation Procedure}
To install the system, the aarch64 Raspberry Pi image was downloaded from the Alpine Linux website
and then flashed onto a microSD card using \texttt{dd}:
\begin{minted}{bash}
sudo dd if=alpine-rpi-*.img of=/dev/sdX bs=4M status=progress
\end{minted}

The built-in \texttt{setup-alpine} script was executed to configure basic settings on the system.
The script guided configuration of essential system settings including:
keyboard layout, hostname, timezone, and network interfaces (using DHCP for simplicity).
The user was prompted to set a root password, select an NTP client,
and choose between diskless mode or installing Alpine to disk.
For persistence, the "sys" mode was selected, writing to the microSD card directly.

After installation, the system was rebooted.
Network access was verified, and the community repository was enabled by editing \texttt{/etc/apk/repositories}.

\subsection{Post-Install Environment Setup}
Once the base system was operational, the \texttt{foot} terminal emulator was installed:
\begin{minted}{bash}
apk add foot
\end{minted}
Foot was selected for its performance and native Wayland support,
making it an ideal terminal for lightweight graphical environments.

KDE Plasma was then installed to provide a full desktop environment:
\begin{minted}{bash}
apk add plasma-desktop sddm xdg-utils
rc-update add sddm
\end{minted}
This choice was deliberate, as a Qt-based graphical frontend was being developed in parallel.
Using KDE ensured consistent toolkit behavior, better integration with Qt components, and a modern user experience with minimal overhead.
