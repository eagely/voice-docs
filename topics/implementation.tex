\renewcommand*\chapterpagestyle{scrheadings}

\chapter{Implementation}

\section{Spring Boot Prototype}
The initial implementation of the backend service was developed in Kotlin using the Spring Boot framework\footnote{Spring Framework \cite{spring}},
a Java-based platform designed to provide foundational infrastructure for web applications.
The service featured a prototype REST endpoint accessible at \texttt{http://localhost:8080/process},
which accepted textual input as an interim solution prior to the integration of audio-processing capabilities.
This endpoint utilized pattern-matching logic to direct weather-related queries to a dedicated weather service,
while all other queries were processed using the GPT-4 language model.

\section{Audio Recording}

\subsection{Initial Idea}
The recording microservice was originally designed with synchronous methods for capturing audio using a CPAL stream.
These methods were exposed through the \texttt{RecordingService} trait, which defined two primary methods: \texttt{start},
to initiate audio capture, and \texttt{stop}, to retrieve the recorded audio bytes.
Initially, these methods were synchronous, requiring the server to block while waiting for the user to finish recording.
However, supporting a remote recording solution necessitated transforming \texttt{RecordingService} into a fully asynchronous trait to facilitate network communication.
Integrating CPAL's inherently synchronous and non-thread-safe \texttt{Stream} into an asynchronous context introduced significant implementation challenges.
CPAL's \texttt{Stream} type is neither \texttt{Send} nor \texttt{Sync}, preventing safe usage across threads and within async contexts.

\subsection{Temporary Unsafe Implementation}
To circumvent the concurrency constraints temporarily, the recording component initially utilized an unsafe wrapper named \texttt{UnsafeSendSync}.
This wrapper forcibly implemented the \texttt{Send} and \texttt{Sync} traits for CPAL's stream, satisfying compiler requirements despite inherent thread-safety violations:

\begin{minted}{rust}
pub struct UnsafeSendSync<T>(pub T);
unsafe impl<T> Sync for UnsafeSendSync<T> {}
unsafe impl<T> Send for UnsafeSendSync<T> {}
\end{minted}

This approach allowed the system to function temporarily by bypassing Rust's concurrency guarantees, but it was inherently risky and only used in production for a single day.
The unsafe implementation was a stopgap measure until a more robust solution could be developed.

\subsection{Isolation of Synchronous Recording}
The final implementation addressed the safety concerns by isolating the synchronous CPAL stream operation into a dedicated synchronous thread, separate from the async interface.
The dedicated thread continuously captures audio samples from CPAL and writes them into a shared ring buffer implemented as a \texttt{VecDeque}.
Access to the buffer is synchronized using a mutex, and an atomic counter keeps track of the total number of samples recorded.

The asynchronous \texttt{LocalRecorder} struct implements the \texttt{RecordingService} trait without directly accessing the CPAL stream.
Instead, it interacts solely with the shared state --- the mutex-protected ring buffer and atomic counter --- to determine which audio data segments should be returned.
Error propagation is handled by transmitting any errors from the synchronous CPAL thread to the asynchronous interface via a Tokio mpsc channel.
This design ensures asynchronous methods can detect and return relevant errors without violating Rust's safety constraints.

\subsection{Wake Word Detection}
Wake words enable hands-free activation of voice assistants through specific phrases.
Commercial systems commonly use phrases like "Hey Siri" or "Okay Google".
The implementation utilizes the Porcupine library\footnote{Porcupine \cite{porcupine}} for wake word detection.
"Ferris" was selected as the wake word, referencing the Rust mascot.

The detection process is implemented through several code components:

\begin{minted}{rust}
fn process_audio_data(
    data: &[f32],
    frame_buffer: &mut Vec<i16>,
    porcupine: &Porcupine,
    wake_word_enabled: &Arc<AtomicBool>,
    is_recording: &Arc<AtomicBool>,
) {
\end{minted}

This function accepts raw audio data and maintains a frame buffer.
A Porcupine detector instance and atomic flags enable state management.
These parameters allow audio processing and recording state control.

\begin{minted}{rust}
for &sample in data {
    let sample_i16 = (sample * 32767.0) as i16;
    frame_buffer.push(sample_i16);
}
\end{minted}

The previously floating-point audio samples are converted to 16-bit integers,
because Porcupine requires 16-bit Pulse Code Modulation (PCM),
which uses a fixed-point representation with 16-bit precision.
Each converted sample is added to the frame buffer for processing.

\begin{minted}{rust}
if frame_buffer.len() >= porcupine.frame_length() as usize {
    if let Ok(keyword_index) = porcupine.process(frame_buffer) {
\end{minted}

When the buffer reaches sufficient size, Porcupine processes it.
The process method returns a keyword index upon successful detection.

\begin{minted}{rust}
if keyword_index >= 0 && wake_word_enabled.load(Ordering::Relaxed) {
    is_recording.store(true, Ordering::Relaxed);
}
frame_buffer.clear();
\end{minted}

A non-negative keyword index indicates successful wake word detection.
If detection is enabled, the system transitions to recording mode.
The frame buffer is cleared afterward to prepare for the next batch.
This cycle continues, allowing continuous monitoring for wake word occurrences.

\section{Speech-to-Text Transcription}

\subsection{Audio Preprocessing}
Raw audio often contains noise and inconsistent volume levels.
Preprocessing techniques, such as spectral subtraction, isolate vocal frequencies and reduce background noise.
Normalization ensures volume consistency.
The cleaned audio is then transformed into numerical features
using methods like the Mel Frequency Cepstrum\footnote{Mel Frequency Cepstral \cite{mfc}} or spectrograms.

\subsection{Neural Network Analysis}
Preprocessed features are input into neural networks trained to map acoustic signals to text.
Recurrent neural networks\footnote{Recurrent Neural Network \cite{rnn}},
particularly Long short-term memory\footnote{Long short-term memory \cite{lstm}} variants, process audio sequentially, retaining context over time.
Transformers, using self-attention\footnote{Self-attention \cite{self-attention}}, model relationships across the entire sequence in parallel.
Hybrid models often combine convolutional layers for phoneme recognition with transformers for broader interpretation.

\subsection{Decoding Ambiguity}
Neural network outputs are inherently probabilistic, which introduces ambiguity in the transcribed text.
To resolve this uncertainty, decoding algorithms integrate statistical language models
that leverage contextual information to select the most likely transcription.
When dealing with variable-length inputs and timing discrepancies,
Connectionist Temporal Classification \footnote{Connectionist Temporal Classification \cite{ctc}}
is employed to collapse repeated predictions and blank labels, providing a preliminary aligned sequence.
This output is then further refined by attention-based models,
which selectively focus on ambiguous segments to produce a more coherent and accurate final transcription.

\subsection{Rust Trait}
The transcription service is defined via a Rust trait:

\begin{minted}{rust}
pub trait TranscriptionService: Send + Sync {
    async fn transcribe(&self, audio: &Bytes) -> Result<String>;
}
\end{minted}

\subsection{OpenAI Whisper}
OpenAI Whisper employs a transformer model trained on 680,000 hours of multilingual data.
Audio is converted into log-Mel spectrograms, which encode phonetic features.
A transformer encoder-decoder processes these to produce text.

Integration in Rust uses \texttt{whisper-rs}:

\begin{minted}{rust}
pub struct LocalWhisperClient {
    context: Arc<WhisperContext>,
}

impl LocalWhisperClient {
    pub fn new(model: impl Into<String>, use_gpu: bool) -> Result<Self> {
        let mut params = WhisperContextParameters::default();
        params.use_gpu = use_gpu;
        let context = Arc::new(WhisperContext::new_with_params(&model.into(), params)?);
        Ok(Self { context })
    }
}
\end{minted}

\section{Parsing}
Parsing is the process of analyzing text input and converting it
into a structured representation of its meaning --- in order to determine which actions should be performed.
This can be accomplished either through rule-based methods
or via Natural Language Understanding (NLU) approaches.
Rule-based parsing involves matching specific patterns in the input ---
using techniques such as regular expressions or Rust's \texttt{match} statement ---
to extract structured data.
In contrast, NLU-based parsing leverages machine learning models,
often using transformer architectures, to detect intent and extract entities from natural language.

\subsection{Action}
An \texttt{Action} encapsulates the parsed result, including intent, entities, and the original input text:

\begin{minted}{rust}
pub struct Action {
    pub intent: Intent,
    pub entities: Vec<Entity>,
    pub text: String,
}
\end{minted}

The intent and entities may optionally carry confidence scores:

\begin{minted}{rust}
pub struct Intent {
    pub name: IntentKind,
    pub confidence: Option<f32>,
}

pub enum IntentKind {
    LlmQuery,
    SetTimer,
    WeatherQuery,
    DecreaseVolume,
    IncreaseVolume,
    SetVolume,
    CloseWindow,
    MaximizeWindow,
    MinimizeWindow,
    SwitchWorkspace,
    ShowDesktop,
    Other(String),
}
\end{minted}

\subsection{Pattern Matching}
A rule-based parser was implemented using Rust's \texttt{match} statement.
It identified known patterns in user input, returning corresponding intents and entities in the form of an \texttt{Action}.

\subsection{NLU-Based Parser}
For more adaptable parsing, an NLU-based solution was implemented using Rasa\footnote{Rasa \cite{rasa}}.
Only the NLU component was used; dialogue handling was external.
Text inputs underwent tokenization and feature extraction before being processed by the DIETClassifier,
a transformer model that performs both intent classification and entity extraction.

Entities were extracted using the BIO tagging scheme, enhanced with spaCy's \texttt{SpacyEntityExtractor}.
spaCy models performed named entity recognition using statistical techniques trained on corpora such as OntoNotes.
The final output included the identified intent with a confidence score and a list of labeled entities.
This output was transformed into an \texttt{Action} object for downstream use.

\section{Geocoding}
The geocoding service translated text-based location names into geographic coordinates.
It processed \texttt{GeocodeResponse} objects from \texttt{GeocodingService}.
Each request required latitude, longitude, and an API key, with optional parameters for \texttt{units}, \texttt{exclude}, and \texttt{lang}.

\section{Timers}
Timers are among the first features associated with a voice assistant,
making proper implementation essential.
Timer functionality is achieved by spawning a new Tokio task that waits for the specified duration;
upon expiration, a desktop notification is sent to indicate that the timer has finished.

\begin{minted}{rust}
pub trait TimerService: Send + Sync {
    async fn set(&self, duration: Duration, description: String) -> Result<String>;
}
\end{minted}
Each timer requires a \texttt{Duration} and a description,
which specifies the output message when the timer completed.

\section{Raspberry Pi Setup}
The voice assistant was developed for deployment on a Raspberry Pi 4,
intended to function as a dedicated node in a smart home environment.
Given the device's limited resources, particular care was taken to reduce local computational load.
Where possible, services were designed with remote execution in mind,
as to not put any heavy load on the low-end hardware.
Alpine Linux\footnote{Alpine Linux\cite{alpine}}
was chosen as the operating system for its minimalism, fast boot times, and low memory usage.

\subsection{Installation Procedure}
To install the system, the aarch64 Raspberry Pi image was downloaded from the Alpine Linux website
and then flashed onto a microSD card using \texttt{dd}:
\begin{minted}{bash}
sudo dd if=alpine-rpi-*.img of=/dev/sdX bs=4M status=progress
\end{minted}

The built-in \texttt{setup-alpine} script was executed to configure basic settings on the system.
The script guided configuration of essential system settings including:
keyboard layout, hostname, timezone, and network interfaces (using DHCP for simplicity).
The user was prompted to set a root password, select an NTP client,
and choose between diskless mode or installing Alpine to disk.
For persistence, the "sys" mode was selected, writing to the microSD card directly.

After installation, the system was rebooted.
Network access was verified, and the community repository was enabled by editing \texttt{/etc/apk/repositories}.

\subsection{Post-Install Environment Setup}
Once the base system was operational, the \texttt{foot} terminal emulator was installed:
\begin{minted}{bash}
apk add foot
\end{minted}
Foot was selected for its performance and native Wayland support,
making it an ideal terminal for lightweight graphical environments.

KDE Plasma was then installed to provide a full desktop environment:
\begin{minted}{bash}
apk add plasma-desktop sddm xdg-utils
rc-update add sddm
\end{minted}
This choice was deliberate, as a Qt-based graphical frontend was being developed in parallel.
Using KDE ensured consistent toolkit behavior, better integration with Qt components, and a modern user experience with minimal overhead.
